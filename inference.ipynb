{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import mod\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data\n",
    "from data.datasets import MedicalDecathlonDataset, BrainTumourDataset\n",
    "from utils.assertions import ensure\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def model_params(model_dir_str):\n",
    "\tmodel_dir = Path(model_dir_str)\n",
    "\tif not model_dir.is_dir():\n",
    "\t\t\n",
    "\t\traise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "\t\n",
    "\tmodel_path = f\"{model_dir}/best_model.pth\"\n",
    "\tcfg = OmegaConf.load(f\"{model_dir}/config.yaml\")\n",
    "\n",
    "\ttry:\n",
    "\t\tpath_parts = model_dir.parts\n",
    "\t\ttask_name = path_parts[-2]\n",
    "\texcept IndexError:\n",
    "\t\traise ValueError(f\"Could not parse architecture/task/name from model_dir: {model_dir}. Expected structure like 'trained_models/arch/task/name'\")\n",
    "\t\n",
    "\tif not isinstance(cfg, DictConfig):\n",
    "\t\traise TypeError(\"cfg must be a DictConfig.\")\n",
    "\t\n",
    "\t\n",
    "\treturn model_path, cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<data.datasets.MedicalDecathlonDataset object at 0x7e9279949840>\n",
      "torch.Size([32, 64, 32])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m model(image_tensor)\n\u001b[1;32m     47\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m---> 49\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m d \u001b[38;5;241m=\u001b[39m dice_coefficient(\n\u001b[1;32m     51\u001b[0m \tpred, label, num_classes\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mnum_classes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     53\u001b[0m results_per_scale[scale] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [d]\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "from utils.metrics import dice_coefficient, dice_coefficient_classes\n",
    "from utils.utils import setup_seed\n",
    "from hydra.utils import instantiate\n",
    "import os\n",
    "from data.datasets import MedicalDecathlonDataset\n",
    "\n",
    "model_dir_str = \"trained_models/ms-unet3d/Task04_Hippocampus/2025-05-05_12-17-39\"\n",
    "\n",
    "model_path, cfg = model_params(model_dir_str)\n",
    "scales = [0, 1, 2, 3, 4]\n",
    "\n",
    "base_dir = Path(cfg.dataset.base_path)\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for scale in scales:\n",
    "\timg_dir = base_dir / \"imagesTs\" / f\"scale{scale}\"\n",
    "\tlbl_dir = base_dir / \"labelsTs\" / f\"scale{scale}\"\n",
    "\timg_dir_sort = sorted(os.listdir(img_dir))\n",
    "\tlbl_dir_sort = sorted(os.listdir(lbl_dir))\n",
    "\tdataset = MedicalDecathlonDataset(cfg, \"test\", img_dir_sort, lbl_dir_sort)\n",
    "\tdatasets[scale] = dataset\n",
    "\n",
    "\n",
    "setup_seed(cfg.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = instantiate(cfg.architecture.path, cfg)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "results_per_scale = {scale: [] for scale in scales}\n",
    "for scale in scales:\n",
    "\tprint(f\"{datasets[scale]}\")\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor image, label in datasets[scale]:\n",
    "\t\t\tprint(label.shape)\n",
    "\t\t\timage_tensor = image.unsqueeze(0).to(device)\n",
    "\t\t\t\n",
    "\t\t\toutput = model(image_tensor)\n",
    "\t\t\t\n",
    "\t\t\tlabel = label.squeeze(0).cpu()\n",
    "\n",
    "\t\t\tpred = torch.argmax(output, dim=1)\n",
    "\t\t\td = dice_coefficient(\n",
    "\t\t\t\tpred, label, num_classes=cfg.dataset.num_classes, ignore_index=0\n",
    "\t\t\t)\n",
    "\t\t\tresults_per_scale[scale] += [d]\n",
    "\n",
    "\n",
    "print(\"Results per scale:\")\n",
    "for scale, results in results_per_scale.items():\n",
    "    print(f\"Scale {scale}: {results}\")\n",
    "    mean_dice = np.mean(results)\n",
    "    std_dice = np.std(results)\n",
    "    print(f\"Mean Dice: {mean_dice:.4f}, Std Dice: {std_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.unet3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MedicalDecathlonDataset, BrainTumourDataset\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet3d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UNet3D\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massertions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensure\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models.unet3d'"
     ]
    }
   ],
   "source": [
    "from ast import mod\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data\n",
    "from data.datasets import MedicalDecathlonDataset, BrainTumourDataset\n",
    "from models.unet3d import UNet3D\n",
    "\n",
    "from models.factory import create_model\n",
    "from utils.assertions import ensure\n",
    "\n",
    "\n",
    "# ------ Change only this for test of a trained model ------\n",
    "model_dir_str = \"trained_models/ms-unet3d/Task04_Hippocampus/2025-04-24_14-52-40\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "model_dir = pathlib.Path(model_dir_str)\n",
    "if not model_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "\n",
    "model_path = f\"{model_dir}/best_model.pth\"\n",
    "cfg = OmegaConf.load(f\"{model_dir}/config.yaml\")\n",
    "\n",
    "\n",
    "try:\n",
    "    path_parts = model_dir.parts\n",
    "    # model_architechture = path_parts[-3]\n",
    "    task_name = path_parts[-2]\n",
    "    inference_model_name = path_parts[-1]\n",
    "except IndexError:\n",
    "    raise ValueError(f\"Could not parse architecture/task/name from model_dir: {model_dir}. Expected structure like 'trained_models/arch/task/name'\")\n",
    "\n",
    "if not isinstance(cfg, DictConfig):\n",
    "    raise TypeError(\"cfg must be a DictConfig.\")\n",
    "\n",
    "if task_name == \"Task04_Hippocampus\":\n",
    "    dataset = MedicalDecathlonDataset(cfg, \"test\")\n",
    "elif task_name == \"Task01_BrainTumour\":\n",
    "    dataset = BrainTumourDataset(cfg, \"test\") \n",
    "else:\n",
    "    raise ValueError(f\"Unknown task name: {task_name}. Expected 'Task04_Hippocampus' or 'Task01_BrainTumour'.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = cfg.active_architecture\n",
    "\n",
    "model = create_model(cfg, model_name).to(device)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)  # Move the model to the appropriate device\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#samples idx list to check: (6,)\n",
    "sample_idx = 0\n",
    "\n",
    "# ... after dataset creation ...\n",
    "print(f\"Dataset size (test phase): {len(dataset)}\")\n",
    "ensure(len(dataset) > 0, Exception, \"Dataset is empty!\")\n",
    "\n",
    "sample_idx = min(len(dataset)-1, 392) \n",
    "image, gt = dataset[sample_idx]\n",
    "# ... rest of the code ...\n",
    "\n",
    "image, gt = dataset[sample_idx]  # image: (C, D, H, W), gt: (D, H, W)\n",
    "\n",
    "# Add batch dimension and move to device\n",
    "image_batch = image.unsqueeze(0).to(device)  # shape: (1, C, D, H, W)\n",
    "print(f\"Image shape: {image_batch.shape}\")\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(image_batch)\n",
    "    # If model returns deep supervision outputs, take the final prediction\n",
    "    if isinstance(output, (tuple, list)):\n",
    "        output = output[0]\n",
    "    # Get predicted labels: (B, D, H, W)\n",
    "    pred = torch.argmax(output, dim=1).squeeze(0).cpu()  \n",
    "\n",
    "# Convert tensors to numpy arrays for visualization\n",
    "# Remove channel dimension from image for visualization: (D, H, W)\n",
    "image_np = image.squeeze(0).cpu().numpy()\n",
    "gt_np = gt.cpu().numpy()\n",
    "pred_np = pred.numpy()\n",
    "\n",
    "# Choose 3 slices evenly spaced along the depth dimension\n",
    "depth = image_np.shape[0]\n",
    "num_slices = min(depth, 66)\n",
    "slice_indices = np.linspace(0, depth-1, num=num_slices, dtype=int)\n",
    "\n",
    "\n",
    "# Create subplots: one row per slice and 3 columns for image, ground truth, and prediction\n",
    "fig, axes = plt.subplots(nrows=num_slices, ncols=3, figsize=(12, 4 * num_slices))\n",
    "for i, slice_idx in enumerate(slice_indices):\n",
    "    slice_2d = image_np[slice_idx]\n",
    "    # slice_2d = np.rot90(slice_2d)\n",
    "    axes[i, 0].imshow(slice_2d, cmap=\"gray\")\n",
    "    axes[i, 0].set_title(f\"Image Slice {slice_idx}\")\n",
    "    \n",
    "    gt_2d = gt_np[slice_idx]\n",
    "    # gt_2d = np.rot90(gt_2d)\n",
    "    axes[i, 1].imshow(gt_2d, cmap=\"grey\", vmin=0, vmax=cfg.dataset.num_classes - 1)\n",
    "\n",
    "    axes[i, 1].set_title(f\"Ground Truth Slice {slice_idx}\")\n",
    "    pred_2d = pred_np[slice_idx]\n",
    "    # pred_2d = np.rot90(pred_2d)\n",
    "    axes[i, 2].imshow(pred_2d, cmap=\"grey\", vmin=0, vmax=cfg.dataset.num_classes - 1)\n",
    "    axes[i, 2].set_title(f\"Prediction Slice {slice_idx}\")\n",
    "\n",
    "    # for ax in axes[i]:\n",
    "    #     ax.axis(\"off\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.__config__ import show\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_prediction(image, gt_mask, pred_mask, slice_idx=None, class_id=1):\n",
    "    gt_binary = (gt_mask == class_id)\n",
    "    pred_binary = (pred_mask == class_id)\n",
    "\n",
    "    if slice_idx is None:\n",
    "        z_coords = np.where(gt_binary)[0]\n",
    "        slice_idx = z_coords[len(z_coords)//2] if len(z_coords) > 0 else gt_mask.shape[0] // 2\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(image[slice_idx], cmap='gray')\n",
    "    plt.title(\"MRI Slice\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(gt_binary[slice_idx], cmap='gray')\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(pred_binary[slice_idx], cmap='gray')\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    error_map = np.logical_xor(gt_binary[slice_idx], pred_binary[slice_idx])\n",
    "    plt.imshow(error_map, cmap='Reds')\n",
    "    plt.title(\"Error Map\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Class {class_id} - Slice {slice_idx}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_prediction(image_np, gt_np, pred_np, class_id=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of images!\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import mod\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data\n",
    "from data.datasets import MedicalDecathlonDataset, BrainTumourDataset\n",
    "from models.ms_unet3d import MSUNet3D\n",
    "\n",
    "# ras+ ORIENTATION\n",
    "# class imbalance\n",
    "\n",
    "# task_name = \"Task04_Hippocampus\"\n",
    "task_name = \"Task01_BrainTumour\"\n",
    "inference_model_name = \"2025-03-31_13-44-04\"\n",
    "\n",
    "model_dir = f\"trained_models/unet3d/{task_name}/{inference_model_name}\"\n",
    "model_path = f\"{model_dir}/best_model.pth\"\n",
    "\n",
    "cfg = OmegaConf.load(f\"{model_dir}/config.yaml\")\n",
    "\n",
    "if not isinstance(cfg, DictConfig):\n",
    "    raise TypeError(\"cfg must be a DictConfig.\")\n",
    "\n",
    "# dataset = MedicalDecathlonDataset(cfg, phase=\"test\")\n",
    "dataset = BrainTumourDataset(cfg, phase='train')\n",
    "\n",
    "model = MSUNet3D(\n",
    "    in_channels=1,\n",
    "    num_classes=cfg.dataset.num_classes,\n",
    "    n_filters=cfg.model.n_filters,\n",
    "    dropout=cfg.training.dropout,\n",
    "    batch_norm=True,\n",
    "    inference_fusion_mode=cfg.model.deep_supervision.inference_fusion_mode,\n",
    "    depth=cfg.model.depth,\n",
    "    deep_supervision_levels=cfg.model.deep_supervision.levels\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predetermined sample and random slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 300\n",
    "\n",
    "image, gt = dataset[sample_idx]  # image: (C, D, H, W), gt: (D, H, W)\n",
    "slice = 104 # or 126\n",
    "\n",
    "# three random slices\n",
    "# Pick three random slices along the depth dimension\n",
    "depth = image.shape[3]\n",
    "slices = np.random.choice(depth, size=3, replace=False)\n",
    "\n",
    "# Plot the slices\n",
    "for i, slice_idx in enumerate(slices):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(image[0, :, :, slice_idx], cmap='gray')\n",
    "    plt.title(f\"Slice {slice_idx}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predetermined sample and slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 300\n",
    "\n",
    "image, gt = dataset[sample_idx]  # image: (C, D, H, W), gt: (D, H, W)\n",
    "slice = 104 # or 126\n",
    "\n",
    "#Skew image\n",
    "\n",
    "\n",
    "plt.imshow(image[0, :, :, slice_idx], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inference(self, x: torch.Tensor):\n",
    "    D, H, W = x.shape[2:]\n",
    "    input_shape = (D, H, W)\n",
    "\n",
    "    def div_shape(shape, factor):\n",
    "        return tuple(s // factor for s in shape)\n",
    "\n",
    "    # build mapping shape -> entry string\n",
    "    shape_to_entry = {self.target_shape: \"enc1\"}\n",
    "    for d in range(1, self.depth):\n",
    "        shape_to_entry[div_shape(self.target_shape, 2**d)] = f\"msb{d}\"\n",
    "\n",
    "    rounded = tuple(2 ** round(np.log2(s)) for s in input_shape)\n",
    "    if rounded not in shape_to_entry:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported input shape {input_shape} (rounded {rounded}). \"\n",
    "            f\"Expected one of: {list(shape_to_entry.keys())}\"\n",
    "        )\n",
    "    entry_gateway = shape_to_entry[rounded]\n",
    "\n",
    "    if entry_gateway == \"enc1\":\n",
    "        # full resolution\n",
    "        out = x\n",
    "        encoder_feats = []\n",
    "        for enc, pool, drop in zip(self.encoders, self.pools, self.enc_dropouts):\n",
    "            out = enc(out)\n",
    "            encoder_feats.append(out)\n",
    "            out = drop(pool(out))\n",
    "\n",
    "        # bottleneck\n",
    "        out = self.bn(out)\n",
    "\n",
    "        # Decoder pathway\n",
    "        for up_conv, decoder, drop in zip(\n",
    "            self.up_convs, self.decoders, self.dec_dropouts\n",
    "        ):\n",
    "            out = up_conv(out)\n",
    "            skip = encoder_feats.pop()\n",
    "            out = torch.cat([out, skip], dim=1)\n",
    "            out = decoder(out)\n",
    "            out = drop(out)\n",
    "\n",
    "        final_out = self.final_conv(out)\n",
    "        return final_out\n",
    "    elif entry_gateway.startswith(\"msb\"):\n",
    "        # lower resolution image\n",
    "        level = int(entry_gateway.replace(\"msb\", \"\"))\n",
    "        msb = self.msb_blocks[level - 1]\n",
    "        out = msb(x)\n",
    "        ms_feats = []\n",
    "        ms_feats.append(out)\n",
    "        out = self.pools[level](out)\n",
    "        out = self.enc_dropouts[level](out)\n",
    "\n",
    "        for enc, pool, drop in zip(\n",
    "            list(self.encoders)[level + 1 :],\n",
    "            list(self.pools)[level + 1 :],\n",
    "            list(self.enc_dropouts)[level + 1 :],\n",
    "        ):\n",
    "            out = enc(out)\n",
    "            ms_feats.append(out)\n",
    "            out = drop(pool(out))\n",
    "\n",
    "        # bottleneck\n",
    "        out = self.bn(out)\n",
    "\n",
    "        num_ups = self.depth - level\n",
    "        # decoder up to match MS scale\n",
    "        for up_conv, dec, drop in zip(\n",
    "            list(self.up_convs)[:num_ups],\n",
    "            list(self.decoders)[:num_ups],\n",
    "            list(self.dec_dropouts)[:num_ups],\n",
    "        ):\n",
    "            out = up_conv(out)\n",
    "            skip = ms_feats.pop()\n",
    "            out = torch.cat([out, skip], dim=1)\n",
    "            out = dec(out)\n",
    "            out = drop(out)\n",
    "\n",
    "        final_out = self.ms_heads[level - 1](out)  # ms_heads not final_conv\n",
    "        return final_out\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown entry point in Multiscale UNet: {entry_gateway}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSimulation 2 - backbone vs our"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
