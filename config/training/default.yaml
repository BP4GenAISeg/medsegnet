# generic training defaults
num_epochs: 80
batch_size: 2
num_workers: 4 # parallel CPU workers to keep the GPU fed
pin_memory: true # page‚Äêlock host memory for faster .to(device) copies
persistent_workers: true
prefetch_factor: 2 # each worker will buffer 2 batches ahead
learning_rate: 3e-4
use_amp: true # mixed precision training
grad_clip_norm: 1.0 # gradient clipping

loss:
  _target_: utils.losses.CombinedLoss
  alpha: 0.35
  ignore_index: 0

optimizer:
  _target_: torch.optim.AdamW
  lr: ${..learning_rate}
  weight_decay: 1e-4

scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  step_size: 30
  gamma: 0.1

early_stopper:
  patience: 15
  verbose: true
  delta: 1e-3
  criterion: "loss"
